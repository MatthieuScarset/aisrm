{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AISRM - AI Sales Representative Matcher Welcome to AISRM AISRM is an innovative AI-powered decision tool designed to accelerate the process of finding and matching the perfect sales representatives for specific business needs. By leveraging advanced machine learning algorithms and data analysis, our application takes the guesswork out of sales team optimization. Our Mission In today's competitive business landscape, finding the right sales talent can make or break a company's success. Traditional methods often rely on intuition and limited data points, leading to costly mistakes and lengthy processes. AISRM addresses this challenge by harnessing the power of artificial intelligence to create data-driven matches between businesses and sales professionals. Our mission is to streamline the decision-making process during presales by accelerating the benchmarking of sales agents. Our intelligent, efficient, and cost-effective solution benefits both managers seeking the best sales representatives for specific deals and sales agents looking for opportunities that align with their skills and expertise. Finding the right sales talent is crucial for business success, but traditional selection methods can be time-consuming and ineffective. AISRM solves this challenge by: Intelligent Matching : Our AI analyzes sales representative performance metrics to identify the best candidates for your specific requirements Key Features \ud83e\udd16 AI-Powered Recommendations : Advanced algorithms that learn from successful matches \ud83d\udcca Performance Analytics : Detailed insights into sales representative capabilities and track records \ud83c\udfaf Precision Matching : Customizable criteria to find representatives that align with your business requirements \ud83d\udcc8 Success Prediction : Forecast potential performance based on historical data Get Started Ready to transform your sales process? Try our app and discover how AI can help you build a winning sales team.","title":"Home"},{"location":"#aisrm-ai-sales-representative-matcher","text":"","title":"AISRM - AI Sales Representative Matcher"},{"location":"#welcome-to-aisrm","text":"AISRM is an innovative AI-powered decision tool designed to accelerate the process of finding and matching the perfect sales representatives for specific business needs. By leveraging advanced machine learning algorithms and data analysis, our application takes the guesswork out of sales team optimization.","title":"Welcome to AISRM"},{"location":"#our-mission","text":"In today's competitive business landscape, finding the right sales talent can make or break a company's success. Traditional methods often rely on intuition and limited data points, leading to costly mistakes and lengthy processes. AISRM addresses this challenge by harnessing the power of artificial intelligence to create data-driven matches between businesses and sales professionals. Our mission is to streamline the decision-making process during presales by accelerating the benchmarking of sales agents. Our intelligent, efficient, and cost-effective solution benefits both managers seeking the best sales representatives for specific deals and sales agents looking for opportunities that align with their skills and expertise. Finding the right sales talent is crucial for business success, but traditional selection methods can be time-consuming and ineffective. AISRM solves this challenge by: Intelligent Matching : Our AI analyzes sales representative performance metrics to identify the best candidates for your specific requirements","title":"Our Mission"},{"location":"#key-features","text":"\ud83e\udd16 AI-Powered Recommendations : Advanced algorithms that learn from successful matches \ud83d\udcca Performance Analytics : Detailed insights into sales representative capabilities and track records \ud83c\udfaf Precision Matching : Customizable criteria to find representatives that align with your business requirements \ud83d\udcc8 Success Prediction : Forecast potential performance based on historical data","title":"Key Features"},{"location":"#get-started","text":"Ready to transform your sales process? Try our app and discover how AI can help you build a winning sales team.","title":"Get Started"},{"location":"about/","text":"About AISRM Project Overview AISRM (AI Sales Representative Matcher) is the culmination of an intensive data science and AI learning journey. This application represents a final team project developed by batch #1945 of the LeWagon Data Science & AI bootcamp 2025, showcasing the practical application of machine learning techniques in solving real-world business challenges. The project commenced on July 10th, 2025, and after four weeks of intensive development, research, and testing, it was presented publicly on August 9th, 2025. The Team Our diverse, international team brings together unique perspectives and complementary skills from various backgrounds in data science, business, and technology. Working collaboratively over four intensive weeks, we combined our expertise to create a comprehensive AI-powered solution. Meet the team (in alphabetical order): Ana de Torres Jurado - Data Scientist & Machine Learning Engineer Andreza Santos - Data Analyst & Business Intelligence Specialist Jonatas Furtado do Amaral - Backend Developer & Data Engineer Matthieu Scarset - Full-Stack Developer & Project Lead Victor Boyer - Frontend Developer & UX/UI Designer Technology & Methodology AISRM was built using cutting-edge machine learning techniques and modern web technologies. Our approach combines: Advanced Machine Learning Models for predictive analytics and pattern recognition Natural Language Processing to analyze sales representative profiles and requirements Data Pipeline Engineering for efficient data processing and model training Modern Web Development frameworks for an intuitive user experience LeWagon Bootcamp Experience The LeWagon Data Science & AI bootcamp provided the foundation for this project, offering intensive training in: - Python programming and data manipulation - Machine learning algorithms and model deployment - Data visualization and statistical analysis - Collaborative development using Git and agile methodologies - Product development from concept to deployment This project represents not just a technical achievement, but also a testament to the power of collaborative learning and the practical application of data science skills in solving real business problems. Made with \u2764\ufe0f by the AISRM team from LeWagon","title":"About"},{"location":"about/#about-aisrm","text":"","title":"About AISRM"},{"location":"about/#project-overview","text":"AISRM (AI Sales Representative Matcher) is the culmination of an intensive data science and AI learning journey. This application represents a final team project developed by batch #1945 of the LeWagon Data Science & AI bootcamp 2025, showcasing the practical application of machine learning techniques in solving real-world business challenges. The project commenced on July 10th, 2025, and after four weeks of intensive development, research, and testing, it was presented publicly on August 9th, 2025.","title":"Project Overview"},{"location":"about/#the-team","text":"Our diverse, international team brings together unique perspectives and complementary skills from various backgrounds in data science, business, and technology. Working collaboratively over four intensive weeks, we combined our expertise to create a comprehensive AI-powered solution. Meet the team (in alphabetical order): Ana de Torres Jurado - Data Scientist & Machine Learning Engineer Andreza Santos - Data Analyst & Business Intelligence Specialist Jonatas Furtado do Amaral - Backend Developer & Data Engineer Matthieu Scarset - Full-Stack Developer & Project Lead Victor Boyer - Frontend Developer & UX/UI Designer","title":"The Team"},{"location":"about/#technology-methodology","text":"AISRM was built using cutting-edge machine learning techniques and modern web technologies. Our approach combines: Advanced Machine Learning Models for predictive analytics and pattern recognition Natural Language Processing to analyze sales representative profiles and requirements Data Pipeline Engineering for efficient data processing and model training Modern Web Development frameworks for an intuitive user experience","title":"Technology &amp; Methodology"},{"location":"about/#lewagon-bootcamp-experience","text":"The LeWagon Data Science & AI bootcamp provided the foundation for this project, offering intensive training in: - Python programming and data manipulation - Machine learning algorithms and model deployment - Data visualization and statistical analysis - Collaborative development using Git and agile methodologies - Product development from concept to deployment This project represents not just a technical achievement, but also a testament to the power of collaborative learning and the practical application of data science skills in solving real business problems. Made with \u2764\ufe0f by the AISRM team from LeWagon","title":"LeWagon Bootcamp Experience"},{"location":"getting-started/","text":"Getting Started Data Source We use a public dataset specifically designed for testing and learning purposes, which includes five CSV files: https://www.kaggle.com/datasets/innocentmfa/crm-sales-opportunities \u251c\u2500\u2500 accounts.csv \u251c\u2500\u2500 data_dictionary.csv \u251c\u2500\u2500 products.csv \u251c\u2500\u2500 sales_pipeline.csv \u2514\u2500\u2500 sales_teams.csv Data Pipeline AISRM provides make commands to streamline all repetitive tasks. Our entire Extract Transform Load (ETL) process can be executed with this simple terminal command: make data Let's break down the step-by-step process: Extract the Dataset Files We use a .env file where RAW_DATA_ARCHIVE and RAW_DATA_URL are specified. # Download and extract raw data from remote source mkdir -p data/raw curl -L -o ./data/raw/$(RAW_DATA_ARCHIVE) $(RAW_DATA_URL) unzip -u data/raw/$(RAW_DATA_ARCHIVE) -d data/raw rm -rf data/raw/$(RAW_DATA_ARCHIVE) Merge Data into a Single CSV File Our custom Python module merges all files by relevant columns (e.g., sales_agent , product , etc.), reorders, removes or renames columns for better usage, and ensures correct data types (e.g., numeric). # Compile the raw dataset for model training python -m src.data The dataset is now ready for model training. Your data folder structure should look like this: data \u251c\u2500\u2500 processed \u2502 \u2514\u2500\u2500 dataset.csv \u2514\u2500\u2500 raw \u251c\u2500\u2500 accounts.csv \u251c\u2500\u2500 data_dictionary.csv \u251c\u2500\u2500 products.csv \u251c\u2500\u2500 sales_pipeline.csv \u2514\u2500\u2500 sales_teams.csv 3 directories, 6 files Model Training We provide a convenient make command to train and save a new model: # Train and save model for development make model This is equivalent to running: python -m src.model dev This command saves a new model, its preprocessor, and metadata as Pickle binary files under models/ . For production, we use model versioning with this command: # Train and save models for deployment make models_prod Behind the scenes, this runs: python -m src.model v1 python -m src.model v2 Your models folder structure should look like this: models \u251c\u2500\u2500 dev-1753300409 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u251c\u2500\u2500 dev-1753300434 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u251c\u2500\u2500 dev-1753807777 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u251c\u2500\u2500 v1 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u2514\u2500\u2500 v2 \u251c\u2500\u2500 metadata.pkl \u251c\u2500\u2500 model.pkl \u2514\u2500\u2500 preprocessor.pkl 6 directories, 15 files Backend Development The backend application handles prediction making and is built with FastAPI. It can run locally and be shipped as a Docker container for production. # Run locally for development # Available at http://localhost:8500 make api_dev # Build the Docker application make api_docker_build # Start and test the application make api_docker_start make test_api # Stop the service make api_docker_stop Frontend Development The frontend application serves as our decision-making tool and is built with Streamlit. It can run locally and be shipped as a Docker container for production. # Run locally for development # Available at http://localhost:8501 make app_dev # Build the Docker application make app_docker_build # Start and test the application make app_docker_start make test_app # Stop the service make app_docker_stop Deployment Backend and frontend applications are decoupled to ensure better performance, scalability, and the ability to deploy individual applications when needed. Deployments are triggered automatically via GitHub workflow when creating a new release tag: # Check latest tags git fetch --tags git tag # Tag must be named release-{app|api}-{0.0.x} # See .github/workflows/deploy.yml -> on.push.tag git tag -a release-app-0.0.1 -m \"Describe this version\" git push --tags # Monitor the GitHub Actions workflow # See https://github.com/MatthieuScarset/aisrm/actions/workflows/deploy.yml Once deployed, you can retrieve the URLs from Google Cloud using these make commands: # Get the deployed API URL make cloud_get_api_url # Get the deployed APP URL make cloud_get_app_url","title":"Getting started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#data-source","text":"We use a public dataset specifically designed for testing and learning purposes, which includes five CSV files: https://www.kaggle.com/datasets/innocentmfa/crm-sales-opportunities \u251c\u2500\u2500 accounts.csv \u251c\u2500\u2500 data_dictionary.csv \u251c\u2500\u2500 products.csv \u251c\u2500\u2500 sales_pipeline.csv \u2514\u2500\u2500 sales_teams.csv","title":"Data Source"},{"location":"getting-started/#data-pipeline","text":"AISRM provides make commands to streamline all repetitive tasks. Our entire Extract Transform Load (ETL) process can be executed with this simple terminal command: make data Let's break down the step-by-step process:","title":"Data Pipeline"},{"location":"getting-started/#extract-the-dataset-files","text":"We use a .env file where RAW_DATA_ARCHIVE and RAW_DATA_URL are specified. # Download and extract raw data from remote source mkdir -p data/raw curl -L -o ./data/raw/$(RAW_DATA_ARCHIVE) $(RAW_DATA_URL) unzip -u data/raw/$(RAW_DATA_ARCHIVE) -d data/raw rm -rf data/raw/$(RAW_DATA_ARCHIVE)","title":"Extract the Dataset Files"},{"location":"getting-started/#merge-data-into-a-single-csv-file","text":"Our custom Python module merges all files by relevant columns (e.g., sales_agent , product , etc.), reorders, removes or renames columns for better usage, and ensures correct data types (e.g., numeric). # Compile the raw dataset for model training python -m src.data The dataset is now ready for model training. Your data folder structure should look like this: data \u251c\u2500\u2500 processed \u2502 \u2514\u2500\u2500 dataset.csv \u2514\u2500\u2500 raw \u251c\u2500\u2500 accounts.csv \u251c\u2500\u2500 data_dictionary.csv \u251c\u2500\u2500 products.csv \u251c\u2500\u2500 sales_pipeline.csv \u2514\u2500\u2500 sales_teams.csv 3 directories, 6 files","title":"Merge Data into a Single CSV File"},{"location":"getting-started/#model-training","text":"We provide a convenient make command to train and save a new model: # Train and save model for development make model This is equivalent to running: python -m src.model dev This command saves a new model, its preprocessor, and metadata as Pickle binary files under models/ . For production, we use model versioning with this command: # Train and save models for deployment make models_prod Behind the scenes, this runs: python -m src.model v1 python -m src.model v2 Your models folder structure should look like this: models \u251c\u2500\u2500 dev-1753300409 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u251c\u2500\u2500 dev-1753300434 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u251c\u2500\u2500 dev-1753807777 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u251c\u2500\u2500 v1 \u2502 \u251c\u2500\u2500 metadata.pkl \u2502 \u251c\u2500\u2500 model.pkl \u2502 \u2514\u2500\u2500 preprocessor.pkl \u2514\u2500\u2500 v2 \u251c\u2500\u2500 metadata.pkl \u251c\u2500\u2500 model.pkl \u2514\u2500\u2500 preprocessor.pkl 6 directories, 15 files","title":"Model Training"},{"location":"getting-started/#backend-development","text":"The backend application handles prediction making and is built with FastAPI. It can run locally and be shipped as a Docker container for production. # Run locally for development # Available at http://localhost:8500 make api_dev # Build the Docker application make api_docker_build # Start and test the application make api_docker_start make test_api # Stop the service make api_docker_stop","title":"Backend Development"},{"location":"getting-started/#frontend-development","text":"The frontend application serves as our decision-making tool and is built with Streamlit. It can run locally and be shipped as a Docker container for production. # Run locally for development # Available at http://localhost:8501 make app_dev # Build the Docker application make app_docker_build # Start and test the application make app_docker_start make test_app # Stop the service make app_docker_stop","title":"Frontend Development"},{"location":"getting-started/#deployment","text":"Backend and frontend applications are decoupled to ensure better performance, scalability, and the ability to deploy individual applications when needed. Deployments are triggered automatically via GitHub workflow when creating a new release tag: # Check latest tags git fetch --tags git tag # Tag must be named release-{app|api}-{0.0.x} # See .github/workflows/deploy.yml -> on.push.tag git tag -a release-app-0.0.1 -m \"Describe this version\" git push --tags # Monitor the GitHub Actions workflow # See https://github.com/MatthieuScarset/aisrm/actions/workflows/deploy.yml Once deployed, you can retrieve the URLs from Google Cloud using these make commands: # Get the deployed API URL make cloud_get_api_url # Get the deployed APP URL make cloud_get_app_url","title":"Deployment"}]}